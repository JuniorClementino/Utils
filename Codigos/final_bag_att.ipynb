{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/junior/.local/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/lib/python3/dist-packages (from pandas) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas) (2018.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/junior/.local/lib/python3.6/site-packages (from pandas) (1.18.1)\n",
      "\u001b[31mERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 517, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/PyYAML-5.1.2.dist-info/METADATA'\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/junior/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/junior/.local/lib/python3.6/site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/junior/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/junior/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/junior/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "\u001b[31mERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 517, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/PyYAML-5.1.2.dist-info/METADATA'\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e6e7ee9b9116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{sys.executable} -m pip install sklearn '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Importando bibliotecas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# import pandas as pd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "#import json\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "#pip install pandas !\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas \n",
    "import sys\n",
    "!{sys.executable} -m pip install sklearn \n",
    "\n",
    "sklearn\n",
    "# Importando bibliotecas\n",
    "# import pandas as pd\n",
    "# import string\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('rslp')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem.porter import *\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from scipy.spatial.distance import cosine\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# !pip install plotly.express\n",
    "# from plotly import graph_objs as go\n",
    "# # Importando bibliotecas\n",
    "# import pandas as pd\n",
    "# import string\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('rslp')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem.porter import *\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from scipy.spatial.distance import cosine\n",
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# !pip install plotly.express\n",
    "# from plotly import graph_objs as go\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "# import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.pipeline import\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading file provide SQL\n",
    "def read_csv(name_df,path):\n",
    "    name_df = pd.read_csv(path+\".csv\",index_col=0)\n",
    "    return name_df\n",
    "#df = pd.read_csv(\"data-1579021640655.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "\n",
    "#deleting columns that have no interest\n",
    "def delete_column_df(df_name,*args):\n",
    "    for name_col in args:\n",
    "        df_name = df_name.drop(columns=[name_col])\n",
    "    return df_name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_csv(name_df, name_file):\n",
    "    name_df.to_csv(namefile+\"csv\")\n",
    "    print(\"File saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to search for attributes in columns\n",
    "def search_att_name(columns_name_json,attribute_json,concept_name_omop):\n",
    "    list_attribute_name = []\n",
    "    for i, row in df.iterrows():\n",
    "        obj = json.loads(row[columns_name_json])\n",
    "    #print(obj)\n",
    "        if(obj[attribute_json] != None):\n",
    "            for procedure_name in obj[attribute_json]:\n",
    "            #print(procedure_name['procedure_ocurrence_concept_name'])\n",
    "                list_attribute_name.append(procedure_name[concept_name_omop])\n",
    "            \n",
    "        else:    \n",
    "            continue\n",
    "    \n",
    "    return list_attribute_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_=search_attribute_name('internacao_json','ocorrencias','condition_ocurrence_concept_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude repeated attributes from the list\n",
    "def exclude_repeated_att(list_att):\n",
    "    update_list = list(dict.fromkeys(list_att))\n",
    "    return update_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to join the list of attributes\n",
    "def join_list(*args):\n",
    "    for lists_ in args:\n",
    "        final_list += lists_\n",
    "    return  final_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fill dataframe, first argument: dataframe name and other lists\n",
    "def fill_columns_data_frame(name_df_fill,*args):\n",
    "    name_df_fill=pd.DataFrame()\n",
    "    for j in args:\n",
    "        for i in j:\n",
    "            name_df_fill[i] = \"\"\n",
    "    return name_df_fill       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lista_ = exclude_repeated_att(lista_)\n",
    "# fill_columns_data_frame('meu_data',lista_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fill dataframe, first argument: dataframe name and other lists. For each desired attribute:\n",
    "#- Arguments:\n",
    "    #--- columns_name_json = column name of the dataframe that is the json\n",
    "    #--- id_att = visit occurrence id\n",
    "    #--- attribute_json = json attribute / variable name\n",
    "    #--- concept_name_omop = name of concept / omop that will be used\n",
    "    \n",
    "def fill_values_do_count_att (columns_name_json,id_att,attribute_json,concept_name_omop ):\n",
    "    for i, row in df.iterrows():\n",
    "        obj = json.loads(row[columns_name_json])\n",
    "        qnt = i\n",
    "        #print(\"Quantas vezes\", i)\n",
    "        #print(obj)\n",
    "        id = str(obj[id_att])\n",
    "        if(obj[attribute_json] != None):\n",
    "            for procedure_name in obj[attribute_json]:\n",
    "                #print(procedure_name['procedure_ocurrence_concept_name'])\n",
    "                nome_do_procedimento = str(procedure_name[concept_name_omop]) \n",
    "                for col in new_df.columns:\n",
    "                    col= str(col)\n",
    "                    if nome_do_procedimento == col:\n",
    "                        #print(nome_do_procedimento + \"-----------\"   + col + \"      Match\" )\n",
    "                        new_df.at[i, 'id'] = id\n",
    "                        #df_test2.loc[df_test2.index[1], col] = \"oi\"\n",
    "                        new_df.at[i, col] = \"OK\"\n",
    "                        #df_test2 = df_test2.append({col : '1'}, index=i)\n",
    "                    else:\n",
    "                        #print(\"Não Deu 1\" +nome_do_procedimento,col)\n",
    "                        #df_test2=df_test2.at[i, col] = \"Vazio\"\n",
    "                        new_df.at[i, 'id'] = id\n",
    "                        continue\n",
    "            \n",
    "        else:\n",
    "            new_df.at[i, 'id'] = id\n",
    "            #print(\"Não Deu 2\")\n",
    "            #df_test2=df_test2.at[i, col] = \"Sem procedimento\"\n",
    "            continue\n",
    "   return new_df         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join two or more dataframe\n",
    "#- args = name of data frames\n",
    "def join_df(df_1,*args):\n",
    "    df_final_join=pd.DataFrame()\n",
    "    for df in args:\n",
    "        df_final_join= pd.concat([df_final_join,df], axis=1, sort=False)\n",
    "    return  df_final_join   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete column or columns from a data frame\n",
    "#   --argument:\n",
    "#    --dataframe name \n",
    "#    --columns names\n",
    "def delete_column_df(df_name,*args):\n",
    "    for name_col in args:\n",
    "        df_name = df_name.drop(columns=[name_col])\n",
    "    return df_name        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate idf:\n",
    "#------argument: an array / numpy\n",
    "\n",
    "# term frequency\n",
    "def calculte_idf(name_array):\n",
    "    smooth_idf = True\n",
    "    norm_idf = True\n",
    "    N = name_array.shape[0]\n",
    "    tf = np.array([name_array[i, :] / np.sum(name_array, axis=1)[i] for i in range(N)])\n",
    "\n",
    "    # inverse documents frequency\n",
    "    df = np.count_nonzero(name_array, axis=0)\n",
    "    idf = np.log((1 + N) / (1 + df)) + 1  if smooth_idf else np.log( N / df )\n",
    "    \n",
    "    #Normalize \n",
    "    tfidf = normalize(tf*idf) if norm_idf else tf*idf\n",
    "    name_df = pd.DataFrame(tfidf, columns=list(df3.columns))\n",
    "    return name_df\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_create_df(idf):\n",
    "    #Normalize \n",
    "    tfidf = normalize(tf*idf) if norm_idf else tf*idf\n",
    "    name_df = pd.DataFrame(tfidf, columns=list(df3.columns))\n",
    "    return name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform dataframe to array_np to calculate idf\n",
    "def df_to_array_np(name_df):\n",
    "    name_array=name_df.to_numpy().astype(np.float)\n",
    "    return name_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= read_csv('df','data-1579021640655')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array2 = fill_values_do_count_att_2('internacao_json','visit_occurrence_id','procedimentos','procedure_ocurrence_concept_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = []\n",
    "# arr_t=[]\n",
    "# string_concept =\"\"\n",
    "# for i, row in df.iterrows():\n",
    "#     obj = json.loads(row['internacao_json'])\n",
    "#     string_concept += obj['visit_concept_name']+\";\"\n",
    "    \n",
    "#     if obj['procedimentos'] != None:\n",
    "#         for procedure_name in obj['procedimentos']:\n",
    "#             nome_do_procedimento = str(procedure_name['procedure_ocurrence_concept_name'])\n",
    "#             string_concept += nome_do_procedimento + \";\"\n",
    "#             arr_t.append(nome_do_procedimento) \n",
    "            \n",
    "            \n",
    "#     elif obj['ocorrencias'] != None:\n",
    "#         for procedure_name in obj['ocorrencias']:\n",
    "#             nome_do_procedimento = str(procedure_name['condition_ocurrence_concept_name'])\n",
    "#             string_concept += nome_do_procedimento + \";\"\n",
    "#             arr_t.append(nome_do_procedimento)\n",
    "            \n",
    "#     elif obj['ocorrencias'] and obj['procedimentos'] == None:\n",
    "#         string_concept+= obj['visit_concept_name']+';'\n",
    "        \n",
    "    \n",
    "#     arr.append(string_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "arr_t=[]\n",
    "count=0\n",
    "for i, row in df.iterrows():\n",
    "    obj = json.loads(row['internacao_json'])\n",
    "    string_concept = ''\n",
    "    arr_t.append(obj['visit_concept_name'])\n",
    "    string_concept+=obj['visit_concept_name']+'&'\n",
    "    #id = str(obj['visit_occurrence_id'])\n",
    "    if(obj['procedimentos'] != None):\n",
    "        for procedure_name in obj['procedimentos']:\n",
    "            nome_do_procedimento = str(procedure_name['procedure_ocurrence_concept_name'])\n",
    "            string_concept += nome_do_procedimento + \"&\"\n",
    "            arr_t.append(nome_do_procedimento)\n",
    "    else:\n",
    "        count+=1\n",
    "          \n",
    "    if(obj['ocorrencias'] != None):\n",
    "        for procedure_name in obj['ocorrencias']:\n",
    "            nome_do_procedimento = str(procedure_name['condition_ocurrence_concept_name'])\n",
    "            string_concept += nome_do_procedimento +'&'\n",
    "            arr_t.append(nome_do_procedimento)\n",
    "    if(obj['ocorrencias'] == None and obj['procedimentos'] == None):\n",
    "        count+=1\n",
    "    \n",
    "    arr.append(string_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df= df1.drop(df1.loc[:, 'CPK ( CREATINOFOSFOQUINASE )':'id'].columns, axis = 1)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "input = arr_t\n",
    "arr_t = [x.lower() for x in input] \n",
    "\n",
    "input1 = arr\n",
    "arr = [x.lower() for x in input1] \n",
    "\n",
    "arr_t = exclude_repeated_att(arr_t)\n",
    "#arr_t\n",
    "print(len(arr_t))\n",
    "\n",
    "arr = [re.sub(' +', ' ', elem) for elem in arr]\n",
    "arr = [re.sub(\"[^A-Za-z\\d\\&]\", \"_\", elem) for elem in arr]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df1 = df1.replace(np.nan, 0.0)\n",
    "#df1 = df1.replace('OK', 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = list(map(lambda s: s.replace('&' , ' '), arr))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x = tfidf.fit_transform(arr)\n",
    "df_tfidf = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n",
    "df_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_tfidf.iterrows():\n",
    "   \n",
    "    if row ['17___alfa_hidroxiprogesterona'] != 0.0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.loc[27961,'17___alfa_hidroxiprogesterona'] + df_tfidf.loc[25304,'17___alfa_hidroxiprogesterona']  + df_tfidf.loc[21726,'17___alfa_hidroxiprogesterona'] + df_tfidf.loc[11693,'17___alfa_hidroxiprogesterona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
